{
 "cells": [
  {
   "cell_type": "raw",
   "id": "fac34d93",
   "metadata": {},
   "source": [
    "Copyright 2023 Benjamin Alexander Albert and Yunxiao Yang [Karchin Lab]\n",
    "All Rights Reserved\n",
    "\n",
    "BigMHC Academic License\n",
    "\n",
    "attention.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48198c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "srcpath = os.path.abspath(\"../src\")\n",
    "if srcpath not in sys.path:\n",
    "    sys.path.append(srcpath)\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from mhcenc import MHCEncoder\n",
    "from dataset import Dataset\n",
    "from bigmhc import BigMHC\n",
    "from mhcuid import mhcuid\n",
    "\n",
    "datadir  = \"../data\"\n",
    "traindir = os.path.join(datadir, \"el_train\")\n",
    "\n",
    "print(\"making MHCEncoder...\", end=\"\")\n",
    "mhcenc = MHCEncoder.read(os.path.join(datadir, \"pseudoseqs.csv\"))\n",
    "print(\"done\")\n",
    "\n",
    "print(\"reading data...\", end=\"\")\n",
    "eltrain = pd.read_csv(os.path.join(datadir, \"el_trainval.csv\"))\n",
    "eltest  = pd.read_csv(os.path.join(datadir, \"el_test.csv\"))\n",
    "print(\"done\")\n",
    "\n",
    "print(\"loading models...\", end=\"\")\n",
    "models = [\n",
    "    torch.nn.DataParallel(BigMHC.load(\n",
    "        \"../models/eltrainval_models/bat{}\".format(int(2**b))).to(0).eval())\n",
    "    for b in range(9, 16)]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93a296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated via https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb on Jan. 3rd, 2023\n",
    "# >HLA-A*66:01\n",
    "# MAVMAPRTLVLLLSGALALTQTWAGSHSMRYFYTSVSRPGRGEPRFIAVGYVDDTQFVRFDSDAASQRMEPRAPWIEQEGPEYWDRNTRNVKAQSQTDRVDLGTLRG\n",
    "# YYNQSEDGSHTIQRMYGCDVGPDGRFLRGYQQDAYDGKDYIALNEDLRSWTAADMAAQITQRKWETAHEAEQWRAYLEGRCVEWLRRYLENGKETLQRTDAPKTHMT\n",
    "# HHAVSDHEATLRCWALSFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWASVVVPSGQEQRYTCHVQHEGLPKPLTLRWEPSSQPTIPIVGIIAGLVLFGAV\n",
    "# IAGAVVAAVMWRRKSSDRKGGSYSQAASSDSAQGSDMSLTACKV\n",
    "\n",
    "# >HLA-B*40:01\n",
    "# MRVTAPRTVLLLLSAALALTETWAGSHSMRYFHTAMSRPGRGEPRFITVGYVDDTLFVRFDSDATSPRKEPRAPWIEQEGPEYWDRETQISKTNTQTYRESLRNLRG\n",
    "# YYNQSEAGSHTLQRMYGCDVGPDGRLLRGHNQYAYDGKDYIALNEDLRSWTAADTAAQISQRKLEAARVAEQLRAYLEGECVEWLRRYLENGKDKLERADPPKTHVT\n",
    "# HHPISDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDRTFQKWAAVVVPSGEEQRYTCHVQHEGLPKPLTLRWEPSSQSTVPIVGIVAGLAVLAVV\n",
    "# VIGAVVAAVMCRRKSSGGKGGSYSQAACSDSAQGSDVSLTA\n",
    "\n",
    "# >HLA-C*08:02\n",
    "# MRVMAPRTLILLLSGALALTETWACSHSMRYFYTAVSRPGRGEPRFIAVGYVDDTQFVQFDSDAASPRGEPRAPWVEQEGPEYWDRETQKYKRQAQTDRVSLRNLRG\n",
    "# YYNQSEAGSHTLQRMYGCDLGPDGRLLRGYNQFAYDGKDYIALNEDLRSWTAADKAAQITQRKWEAAREAEQRRAYLEGTCVEWLRRYLENGKKTLQRAEHPKTHVT\n",
    "# HHPVSDHEATLRCWALGFYPAEITLTWQRDGEDQTQDTELVETRPAGDGTFQKWAAVVVPSGEEQRYTCHVQHEGLPEPLTLRWGPSSQPTIPIVGIVAGLAVLAVL\n",
    "# AVLGAVMAVVMCRRKSSGGKGGSCSQAASSNSAQGSDESLIACKA\n",
    "\n",
    "# HLA-A*02:01\n",
    "# https://www.rcsb.org/structure/7RTD\n",
    "\n",
    "# HLA-B*07:02\n",
    "# https://www.rcsb.org/structure/6AT5\n",
    "\n",
    "# HLA-C*07:02\n",
    "# https://www.rcsb.org/structure/6PAG\n",
    "\n",
    "hlaA_alphafold = \"HLAA6601\"\n",
    "hlaB_alphafold = \"HLAB4001\"\n",
    "hlaC_alphafold = \"HLAC0802\"\n",
    "\n",
    "hlaA_bound = \"HLAA0201\"\n",
    "hlaB_bound = \"HLAB0702\"\n",
    "hlaC_bound = \"HLAC0702\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d074bbd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getMHCAttention(tmpfp, allele):\n",
    "    pmhcs = Dataset.readPMHCs(\n",
    "            fp=tmpfp,\n",
    "            allele=allele,\n",
    "            pepcol=0,\n",
    "            verbose=False)\n",
    "    dataset = Dataset(pmhcs=pmhcs, mhcenc=mhcenc)\n",
    "    dataset.makebats(maxbat=72000)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=None,\n",
    "        shuffle=False,\n",
    "        num_workers=1,\n",
    "        prefetch_factor=1)\n",
    "    attention = None\n",
    "    with torch.no_grad():\n",
    "        for idx,bat in enumerate(loader):\n",
    "            for m in models:\n",
    "                prob, att = m(\n",
    "                    mhc=bat.mhc.to(0),\n",
    "                    pep=bat.pep.to(0))\n",
    "                att = torch.sum(att,dim=0).cpu()\n",
    "                if attention is None:\n",
    "                    attention = att\n",
    "                else:\n",
    "                    attention += att\n",
    "    return (attention / len(pmhcs) / len(models)).tolist()\n",
    "\n",
    "\n",
    "def getMHCSingleAttention(allele, peptide):\n",
    "    tmpfp = \"cache/tmp.csv\"\n",
    "    with open(tmpfp, \"w\") as f:\n",
    "        f.write(f\"{peptide}\")\n",
    "    return getMHCAttention(tmpfp, allele)\n",
    "\n",
    "    \n",
    "def getMHCMeanAttention(df):\n",
    "    tmpfp = \"cache/tmp.csv\"\n",
    "    df.iloc[:,1].to_csv(tmpfp, index=False, header=False)\n",
    "    return getMHCAttention(tmpfp, df.iloc[0,0])\n",
    "\n",
    "\n",
    "def getMeanAttention(df, cache):\n",
    "    if os.path.isfile(cache):\n",
    "        print(\"reading cache file {}...\".format(cache), end=\"\")\n",
    "        attention = pd.read_pickle(cache)\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        if not os.path.exists(os.path.dirname(cache)):\n",
    "            os.makedirs(os.path.dirname(cache))\n",
    "        print(\"processing {} pMHC instances...\".format(len(df)))\n",
    "        alleles = sorted(df.iloc[:,0].unique())\n",
    "        attention = list()\n",
    "        for idx,mhc in enumerate(alleles):\n",
    "            print(\"  {}/{} {}...\".format(idx+1,len(alleles),mhc), end=\"\")\n",
    "            mhcdf = df[df.iloc[:,0]==mhc]\n",
    "            attention.append([mhc,0,getMHCMeanAttention(mhcdf[mhcdf.iloc[:,2]==0])])\n",
    "            attention.append([mhc,1,getMHCMeanAttention(mhcdf[mhcdf.iloc[:,2]==1])])\n",
    "            print(\"done\")\n",
    "        attention = pd.DataFrame(\n",
    "            columns=[\"mhc\",\"tgt\",\"att\"],\n",
    "            data=attention)\n",
    "        attention.to_pickle(cache)\n",
    "    return attention\n",
    "\n",
    "\n",
    "cacheMeanTrain = \"cache/el_mean_att_train.pkl\"\n",
    "cacheMeanTest  = \"cache/el_mean_att_test.pkl\"\n",
    "\n",
    "trainMeanAttention = getMeanAttention(eltrain, cacheMeanTrain)\n",
    "testMeanAttention  = getMeanAttention(eltest,  cacheMeanTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b193350",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def makeAttentionHeatmaps(df, attention, title):\n",
    "    \n",
    "    df.iloc[:,0] = df.iloc[:,0].apply(mhcuid)\n",
    "    attention[\"mhc\"] = attention[\"mhc\"].apply(mhcuid)\n",
    "    \n",
    "    neg = {'A':[], 'B':[], 'C':[]}\n",
    "    pos = {'A':[], 'B':[], 'C':[]}\n",
    "    dif = {'A':[], 'B':[], 'C':[]}\n",
    "    \n",
    "    alldiff = dict()\n",
    "    \n",
    "    yticklabels = {'A':[], 'B':[], 'C':[]}\n",
    "    \n",
    "    alleles = sorted(df.iloc[:,0].unique())\n",
    "\n",
    "    for mhc in alleles:\n",
    "        if not mhc.startswith(\"HLA\"):\n",
    "            continue\n",
    "        rows = attention[attention[\"mhc\"]==mhc]\n",
    "        N = torch.tensor(rows[rows[\"tgt\"]==0][\"att\"].iloc[0])\n",
    "        P = torch.tensor(rows[rows[\"tgt\"]==1][\"att\"].iloc[0])\n",
    "        D = P-N\n",
    "        alldiff[mhc] = D.tolist()\n",
    "\n",
    "        neg[mhc[3]].append(N.tolist())\n",
    "        pos[mhc[3]].append(P.tolist())\n",
    "        dif[mhc[3]].append(D.tolist())\n",
    "\n",
    "        yticklabels[mhc[3]].append(mhc[3:])\n",
    "\n",
    "    neg = {k:torch.tensor(v) for k,v in neg.items()}\n",
    "    pos = {k:torch.tensor(v) for k,v in pos.items()}\n",
    "    dif = {k:torch.tensor(v) for k,v in dif.items()}\n",
    "\n",
    "    absmax = {\n",
    "        x:torch.concat((neg[x],pos[x],dif[x]), dim=1).flatten().quantile(0.99)\n",
    "        for x in list(\"ABC\")}\n",
    "    absmax = {\n",
    "        x:max(absmax.values()) for x in list(\"ABC\")}\n",
    "\n",
    "    xticklabels = list()\n",
    "    for x in [x.split('_')[0] for x in mhcenc.cols]:\n",
    "        if len(xticklabels) and xticklabels[-1]==x:\n",
    "            continue\n",
    "        xticklabels.append(x)\n",
    "\n",
    "\n",
    "    sns.set_context(\"paper\")\n",
    "    style=\"whitegrid\"\n",
    "    rc={\"axes.facecolor\": \"0.99\",\n",
    "        \"grid.color\": \"0.95\",\n",
    "        \"axes.edgecolor\": \"0.90\"}\n",
    "\n",
    "    nA = len(yticklabels['A'])\n",
    "    nB = len(yticklabels['B'])\n",
    "    nC = len(yticklabels['C'])\n",
    "\n",
    "\n",
    "    with sns.axes_style(style=style, rc=rc):\n",
    "\n",
    "        heatmaps = plt.figure(figsize=(16,24*len(alleles)/149), dpi=600)\n",
    "        gs = heatmaps.add_gridspec(\n",
    "            nrows=3,\n",
    "            ncols=4,\n",
    "            wspace=0.02,\n",
    "            hspace=0.01,\n",
    "            width_ratios=[0.32,0.32,0.32,0.04],\n",
    "            height_ratios=[\n",
    "                nA / (nA+nB+nC),\n",
    "                nB / (nA+nB+nC),\n",
    "                nC / (nA+nB+nC)])\n",
    "        ax = gs.subplots()\n",
    "\n",
    "        mapdata = [pos, neg, dif]\n",
    "\n",
    "        for y,sup in enumerate(list(\"ABC\")):\n",
    "            for x,data in enumerate(mapdata):\n",
    "\n",
    "                sns.heatmap(\n",
    "                    data=data[sup],\n",
    "                    ax=ax[y][x],\n",
    "                    cmap=\"RdBu\",\n",
    "                    xticklabels=xticklabels,\n",
    "                    yticklabels=yticklabels[sup] if not x else [],\n",
    "                    vmin=-absmax[sup],\n",
    "                    vmax=absmax[sup],\n",
    "                    cbar_ax=ax[y][3])\n",
    "\n",
    "                if y==0:\n",
    "                    if x==0:\n",
    "                        ax[y][x].set_title(\"Positives\")\n",
    "                    elif x==1:\n",
    "                        ax[y][x].set_title(\"Negatives\")\n",
    "                    else:\n",
    "                        ax[y][x].set_title(\"Differences\")\n",
    "                if x==0:\n",
    "                    ax[y][x].set_ylabel(\"HLA-{}\".format(sup))\n",
    "                \n",
    "        heatmaps.suptitle(title, fontsize=20, y=0.91 if len(alleles)==149 else 1)\n",
    "    \n",
    "    return heatmaps, alldiff\n",
    "\n",
    "\n",
    "savedir = \"figs\"\n",
    "\n",
    "if not os.path.exists(savedir):\n",
    "    os.makedirs(savedir)\n",
    "\n",
    "trainmaps, trainmapsdif = makeAttentionHeatmaps(\n",
    "    df=eltrain,\n",
    "    attention=trainMeanAttention,\n",
    "    title=\"Weighted Mean Attention Values Across HLA Alleles in Training Data\")\n",
    "\n",
    "trainmaps.savefig(os.path.join(savedir,\"train_att_map.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "testmaps, testmapsdif = makeAttentionHeatmaps(\n",
    "    df=eltest,\n",
    "    attention=testMeanAttention,\n",
    "    title=\"Weighted Mean Attention Values Across HLA Alleles in Evaluation Data\")\n",
    "\n",
    "testmaps.savefig(os.path.join(savedir,\"test_att_map.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781a7473-175a-4ed2-a805-549b3acd3e24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def getMultipleSeqAlignment(mhc):\n",
    "    renamed  = f\"{mhc[:3]}-{mhc[3]}\\\\\\*{mhc[4:6]}:{mhc[6:8]}\"\n",
    "    outputs  = subprocess.run(f\"grep {renamed} ../data/seq/mhc.pretty\", shell=True, capture_output=True).stdout\n",
    "    tokens   = str(outputs).split(\"\\\\n\")\n",
    "    fragment = [o.strip().split(\" \")[-1] for o in tokens[:-1]] # Last one is an extra quotation mark\n",
    "    sequence = \"\".join(fragment)\n",
    "    return sequence\n",
    "    \n",
    "print(getMultipleSeqAlignment(hlaA_alphafold))\n",
    "print(getMultipleSeqAlignment(hlaB_alphafold))\n",
    "print(getMultipleSeqAlignment(hlaC_alphafold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adb532",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import py3Dmol\n",
    "import matplotlib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getResAtt(att, mhc, peptide):\n",
    "    if peptide is None:\n",
    "        att = att[mhc]\n",
    "    else:\n",
    "        att = getMHCSingleAttention(mhc, peptide)\n",
    "    alignment = getMultipleSeqAlignment(mhc)\n",
    "    \n",
    "    res = (-1e9) * torch.ones(len(mhcenc[mhc]))\n",
    "    attidx = 0\n",
    "    for x in range(len(res)):\n",
    "        if mhcenc[mhc][x]:\n",
    "            col = int(mhcenc.cols[x].split('_')[0])\n",
    "            colBeforeAlignment = col - alignment[:col].count(\"-\")\n",
    "            res[col] = att[attidx]\n",
    "            attidx += 1\n",
    "            \n",
    "    return res.tolist()\n",
    "\n",
    "\n",
    "def plot3d(pdb, mapsdiff, mhc, q, absmax, startidx=0, endidx=None, peptide=None):\n",
    "    \n",
    "    mhc = mhcuid(mhc)\n",
    "    \n",
    "    att = getResAtt(mapsdiff, mhc, peptide)\n",
    "                            \n",
    "    att = [(x+absmax) / (2*absmax) for x in att]\n",
    "    \n",
    "    with open(pdb) as f:\n",
    "        system = \"\".join([x for x in f])\n",
    "\n",
    "    view = py3Dmol.view(width=600, height=600)\n",
    "    view.addModelsAsFrames(system)\n",
    "    \n",
    "    cmap = matplotlib.cm.get_cmap(\"cool_r\")\n",
    "    cmap = [(cmap(att[x]) if att[x] > -1 else (0.9,0.9,0.9)) for x in range(len(att))]\n",
    "    cmap = [\"#{:02X}{:02X}{:02X}\".format(\n",
    "        int(c[0]*255),\n",
    "        int(c[1]*255),\n",
    "        int(c[2]*255)) for c in cmap]\n",
    "\n",
    "    for res in range(0, endidx-startidx):\n",
    "        style = {\"cartoon\": {\"color\":cmap[res+startidx], \"arrows\":\"True\"}}\n",
    "        # if att[res] >= 0:\n",
    "        #     style[\"stick\"] = {}\n",
    "        view.setStyle({\"resi\":res+1, \"chain\":\"A\"}, style)\n",
    "        \n",
    "    view.setStyle({\"chain\":\"B\"}, {\"cartoon\": {\"color\": cmap[0], \"arrows\":\"True\"}})\n",
    "    view.setStyle({\"chain\":\"C\"}, {\"cartoon\": {\"color\": \"#CD5C5C\", \"arrows\":\"True\"}})\n",
    "    \n",
    "    view.zoomTo()\n",
    "    \n",
    "    alpha = 2*np.arccos(q[3])\n",
    "    sin = np.sin(alpha) / 2\n",
    "    x = q[0] / sin\n",
    "    y = q[1] / sin\n",
    "    z = q[2] / sin\n",
    "    norm = np.linalg.norm([x,y,z])\n",
    "    x /= norm\n",
    "    y /= norm\n",
    "    z /= norm\n",
    "    view.rotate(np.degrees(alpha), {'x':x,'y':y,'z':z})\n",
    "    \n",
    "    view.zoom(q[4])\n",
    "    \n",
    "    view.show()\n",
    "    \n",
    "    view.clim(-4,4)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "to get the viewer quaternion, first execute the following in the web console:\n",
    "\n",
    ">>  viewer = \"\"\n",
    "    for (var name in this)\n",
    "      if (name.startsWith(\"viewer_\"))\n",
    "        viewer = name\n",
    "    console.log(viewer)\n",
    "\n",
    "\n",
    "which will print the name of the most recent viewer, like this:\n",
    "    viewer_16548849854127703\n",
    "\n",
    "then, execute:\n",
    ">>  viewer_16548849854127703.getView()\n",
    "\n",
    "which will return an array of 8 elements, of which the last four can be\n",
    "copy/pasted as an argument to getResAtt as seen below,\n",
    "and the zoom argument (q[4]) can be taken from the fourth element\n",
    "\n",
    "this can be used to manually rotate the view and then hardcode the\n",
    "quaternion to have replicatable viewing angles.\n",
    "\"\"\"\n",
    "\n",
    "hlaAatt = trainmapsdif[hlaA_alphafold]\n",
    "hlaBatt = trainmapsdif[hlaB_alphafold]\n",
    "hlaCatt = trainmapsdif[hlaC_alphafold]\n",
    "\n",
    "minatt = min([*hlaAatt, *hlaBatt, *hlaCatt])\n",
    "maxatt = max([*hlaAatt, *hlaBatt, *hlaCatt])\n",
    "\n",
    "absmax = max(abs(minatt), abs(maxatt))\n",
    "\n",
    "plot3d(\"pdb/hlaa0201.pdb\", trainmapsdif, hlaA_bound,\n",
    "       [-0.10403314577265341, 0.7042097241390635, 0.3536571077838125, 0.6067886115630541, 27],\n",
    "       absmax, startidx=23, endidx=300, peptide=\"YLQPRTFLL\")\n",
    "\n",
    "plot3d(\"pdb/hlab0702.pdb\", trainmapsdif, hlaB_bound,\n",
    "       [-0.6479111973748177, -0.7154232754776874, -0.2530340316945592, 0.06598784755926368, 1.5],\n",
    "       absmax, startidx=24, endidx=300, peptide=\"APRGPHGGAASGL\")\n",
    "\n",
    "plot3d(\"pdb/hlac0702.pdb\", trainmapsdif, hlaC_bound,\n",
    "       [-0.053526661402330956, -0.9915178946404356, 0.03410936344611555, 0.11341830739443379, 1.4],\n",
    "       absmax, startidx=23, endidx=302, peptide=\"RYRPGTVAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9ad54-da20-4046-a1ca-6ef4da45704a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot3d(\"pdb/hlaa6601.pdb\", trainmapsdif, hlaA_alphafold,\n",
    "       [0.01584834947434103, -0.9626169283722876, 0.18212611674386536, 0.19986884857285847, 2.4],\n",
    "        absmax, startidx=0, endidx=366)\n",
    "\n",
    "plot3d(\"pdb/hlab4001.pdb\", trainmapsdif, hlaB_alphafold,\n",
    "       [0.14017763651136617, -0.9146756780073745, 0.2694812537241472, 0.26664299761122323, 2.5],\n",
    "        absmax, startidx=0, endidx=366)\n",
    "\n",
    "plot3d(\"pdb/hlac0802.pdb\", trainmapsdif, hlaC_alphafold,\n",
    "       [0.004088738817461506, -0.9745425250220223, 0.1251330588426521, 0.18598888870915176, 2.5],\n",
    "        absmax, startidx=0, endidx=366)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4b59f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "\n",
    "    \n",
    "# Adapted from https://stackoverflow.com/a/16599889\n",
    "\n",
    "pl.figure(figsize=(1, 12), dpi=600)\n",
    "pl.imshow(np.array([[-absmax,absmax]]), cmap=\"cool_r\")\n",
    "pl.gca().set_visible(False)\n",
    "cax = pl.axes([0.1, 0.1, 0.5, 0.5])\n",
    "pl.colorbar(orientation=\"vertical\", cax=cax)\n",
    "pl.savefig(os.path.join(savedir, \"colorbar.png\"), facecolor=\"white\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
