{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9afd08e3",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Copyright 2023 Benjamin Alexander Albert \\[Karchin Lab\\]\n",
    "\n",
    "All Rights Reserved\n",
    "\n",
    "BigMHC Academic License\n",
    "\n",
    "makedata.ipynb\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### Dataset Compilation\n",
    "\n",
    "Create a dir, which we will call `data`\n",
    "  * Create a dir within `data` called `src`\n",
    "    * Store all of the below downloadables in the `src` dir\n",
    "  * All generated datasets will be placed in a dir called `out` within the `data` dir\n",
    "\n",
    "Download the **NetMHCpan-4.1** datasets from https://services.healthtech.dtu.dk/suppl/immunology/NAR_NetMHCpan_NetMHCIIpan/\n",
    "  * Download NetMHCpan_train.tar.gz, extract it, and rename the dir to `netmhcpan_train`\n",
    "  * Download each of the files in the “MS Ligands” section and copy each allele-specific file into a single space-delimited text file called `netmhcpan_test.txt`\n",
    "\n",
    "Download the **MHCflurry-2.0** datasets from https://doi.org/10.17632/zx3kjzc3yx.3\n",
    "  * Download Data_S2.csv.gz, gunzip it, and rename the csv file to `mhcflurry_s2.csv`\n",
    "  * Download Data_S3.csv and name it `mhcflurry_s3.csv`\n",
    "  * Download Data_S5.csv and name it `mhcflurry_s5.csv`\n",
    "  * Download Data_S6.csv and name it `mhcflurry_s6.csv`\n",
    "\n",
    "Download **PRIME-1.0** dataset from https://doi.org/10.1016/j.xcrm.2021.100194\n",
    "  * Download Table S1 and name it `prime1.xlsx`\n",
    "\n",
    "Download **PRIME-2.0** and **MixMHCpred-2.2** datasets from https://doi.org/10.1101/2022.05.23.492800\n",
    "  * Download supplements/492800_file04.zip and unzip it\n",
    "    * Rename DatasetS3_PRIME.xlsx to `prime2.xlsx`\n",
    "    * Rename DatasetS2_ligands.txt to `mixmhcpred.txt`\n",
    "\n",
    "Download **MHCnuggets** dataset from https://github.com/KarchinLab/mhcnuggets/tree/master/mhcnuggets/data/production/mhcI\n",
    "  * Download curated_training_data.csv and rename it to `mhcnuggets.csv`\n",
    "\n",
    "Download **TransPHLA** dataset from https://github.com/a96123155/TransPHLA-AOMP/tree/master/Dataset\n",
    "  * Download train_set.zip, unzip it, and the csv file to `transphla.csv`\n",
    "\n",
    "Download **HLAthena** dataset from https://doi.org/10.1038/s41587-019-0322-9\n",
    "  * Download supplementary dataset 1 and rename it to `hlathena.xlsx`\n",
    "\n",
    "Download **NEPdb** dataset from http://nep.whu.edu.cn/\n",
    "  * Search individually for HLA-A/B/C, download the results, and name the files `nepdb_a.csv`, `nepdb_b.csv`, and `nepdb_c.csv`\n",
    "  * Downloaded files on December 18, 2022\n",
    "\n",
    "Download **Neopepsee** dataset from https://doi.org/10.1093/annonc/mdy022\n",
    "  * Download the supplementary data zip file and extract the contents\n",
    "  * Rename Supplemental_Table_S5 to `neopepsee.xlsx`\n",
    "\n",
    "Download **TESLA** immunogenicity dataset from: https://doi.org/10.1016/j.cell.2020.09.015\n",
    "  * Download Table S4 and name it `tesla_s4.xlsx`\n",
    "  * Download Table S7 and name it `tesla_s7.xlsx`\n",
    "\n",
    "Download **IEDB** infectious disease immunogenicity data from http://www.iedb.org/\n",
    "  * Query with parameters: (epitope) linear peptide, (assay) T cell with both positive and negative outcomes, (MHC restriction) class I, (Host) human, (Disease) infectious\n",
    "  * After searching, select the “Assays” tab and then click “Export Results”\n",
    "  * Extract the zipped folder and rename the enclosed file to `iedb.csv`\n",
    "  * Downloaded files on December 19, 2022\n",
    "\n",
    "Download the **MANAFEST** dataset from https://doi.org/10.17632/dvmz6pkzvb.1 called `manafest.csv`\n",
    "\n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9913449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b03903",
   "metadata": {},
   "source": [
    "#### Only parameter to set is the path to the datadir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "284c5e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = os.path.join(os.pardir, \"data\")\n",
    "\n",
    "srcpath = os.path.join(datadir, \"src\")\n",
    "outpath = os.path.join(datadir, \"out\")\n",
    "\n",
    "\n",
    "def filterlen(df, minlen=8, maxlen=15):\n",
    "    \"\"\"remove peptides of length fewer than 8 or greater than 15 residues\"\"\"\n",
    "    return df[df.pep.apply(lambda x: (len(x) >= minlen) and (len(x) <= maxlen))].copy()\n",
    "\n",
    "\n",
    "def uid(df):\n",
    "    \"\"\"set index as mhc_pep\"\"\"\n",
    "    df[\"uid\"] = df.mhc + '_' + df.pep\n",
    "    return df.set_index(\"uid\", drop=True)\n",
    "\n",
    "\n",
    "def dedup(df):\n",
    "    \"\"\"remove contradictory pos/neg instances and then deduplicate\"\"\"\n",
    "    # first copy all sorted duplicates and remove them from df\n",
    "    dupidx = df.index.duplicated(keep=False)\n",
    "    dups = df[dupidx].copy().sort_index()\n",
    "    df = df[~dupidx]\n",
    "    # then remove any contradictory pos/neg instances in the dups\n",
    "    dups[\"rm\"] = 0\n",
    "    x = 0\n",
    "    while x < len(dups)-1:\n",
    "        rm = 0\n",
    "        for y in range(x+1,len(dups)):\n",
    "            if dups.index[x] != dups.index[y]:\n",
    "                break\n",
    "            if dups.tgt[x] != dups.tgt[y]:\n",
    "                rm = 1\n",
    "        dups.iloc[x:y,-1] = rm\n",
    "        x = y\n",
    "    dups = dups[dups.rm==0]\n",
    "    # lastly deduplicate and concat\n",
    "    dups = dups[~dups.index.duplicated()]\n",
    "    dups.drop([\"rm\"], axis=1, inplace=True)\n",
    "    return pd.concat((df,dups)).sort_index()\n",
    "\n",
    "\n",
    "def standardizeDF(df):\n",
    "    \"\"\"\n",
    "    Function to be called on every loaded dataset for\n",
    "    standardizing MHC and peptide values and deduplication.\n",
    "    Assumes dataframe has columns: mhc, pep, tgt\n",
    "    \"\"\"\n",
    "    df.mhc = df.mhc.apply(str.upper)\n",
    "    df.pep = df.pep.apply(str.upper)\n",
    "    df.tgt = df.tgt.astype(int)\n",
    "    df = dedup(uid(filterlen(df)))\n",
    "    return df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf9599",
   "metadata": {},
   "source": [
    "#### Load NetMHCpan-4.1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ad22e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NetMHCpan-4.1 training data...\n",
      "removing non-HLA pMHCs...\n",
      "standardizing allele names...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load NetMHCpan-4.1 training data into var: netmhcpanTrain\n",
    "print(\"loading NetMHCpan-4.1 training data...\")\n",
    "netmhcpanTrain = \\\n",
    "    [pd.read_csv(\n",
    "        os.path.join(\n",
    "            srcpath,\n",
    "            \"netmhcpan_train\",\n",
    "            \"c00{}_el\".format(x)),\n",
    "        delimiter=' ',\n",
    "        header=None,\n",
    "        names=[\"pep\",\"tgt\",\"mhc\"])\n",
    "    for x in range(5)]\n",
    "netmhcpanTrain = pd.concat(netmhcpanTrain)\n",
    "print(\"removing non-HLA pMHCs...\")\n",
    "netmhcpanTrain = netmhcpanTrain[netmhcpanTrain.mhc.apply(lambda x: x.startswith(\"HLA\"))]\n",
    "print(\"standardizing allele names...\")\n",
    "netmhcpanTrain.mhc = netmhcpanTrain.mhc.apply(lambda x: \"HLA-{}*{}:{}\".format(x[4], x[5:7], x[-2:]))\n",
    "netmhcpanTrain = netmhcpanTrain[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "print(\"standardizing dataframe...\")\n",
    "netmhcpanTrain = standardizeDF(netmhcpanTrain)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1ad3587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NetMHCpan-4.1 testing data...\n",
      "standardizing allele names...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load NetMHCpan-4.1 testing data into var: netmhcpanTest\n",
    "print(\"loading NetMHCpan-4.1 testing data...\")\n",
    "netmhcpanTest = pd.read_csv(\n",
    "    os.path.join(srcpath, \"netmhcpan_test.txt\"),\n",
    "    delimiter=' ',\n",
    "    header=None,\n",
    "    names=[\"pep\",\"tgt\",\"mhc\"])\n",
    "print(\"standardizing allele names...\")\n",
    "netmhcpanTest.mhc = netmhcpanTest.mhc.apply(lambda x: \"HLA-{}*{}:{}\".format(x[4], x[5:7], x[-2:]))\n",
    "netmhcpanTest = netmhcpanTest[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "print(\"standardizing dataframe...\")\n",
    "netmhcpanTest = standardizeDF(netmhcpanTest)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedd6d70",
   "metadata": {},
   "source": [
    "#### Load MHCflurry-2.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d49b7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MHCflurry-2.0 training data...\n",
      "loading S2...\n",
      "loading S5...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load MHCflurry-2.0 EL training data into var: mhcflurryTrain\n",
    "print(\"loading MHCflurry-2.0 training data...\")\n",
    "cols = [\"hla\", \"peptide\", \"hit\"]\n",
    "print(\"loading S2...\")\n",
    "s2 = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcflurry_s2.csv\"),\n",
    "    usecols=cols)\n",
    "print(\"loading S5...\")\n",
    "s5 = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcflurry_s5.csv\"),\n",
    "    usecols=cols)\n",
    "s2 = s2[cols]\n",
    "s5 = s5[cols]\n",
    "s2.columns = [\"mhc\",\"pep\",\"tgt\"]\n",
    "s5.columns = [\"mhc\",\"pep\",\"tgt\"]\n",
    "mhcflurryTrain = pd.concat((s2, s5))\n",
    "del s2, s5\n",
    "print(\"standardizing dataframe...\")\n",
    "mhcflurryTrain = standardizeDF(mhcflurryTrain)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c74dd2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading other MHCflurry-2.0 data...\n",
      "loading S3...\n",
      "loading S5...\n",
      "loading S6...\n",
      "exploding S6 multiallelic peptides...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load other MHCflurry-2.0 training data: mhcflurryOther\n",
    "print(\"loading other MHCflurry-2.0 data...\")\n",
    "print(\"loading S3...\")\n",
    "s3 = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcflurry_s3.csv\"),\n",
    "    usecols=[\"allele\", \"peptide\"])\n",
    "print(\"loading S5...\")\n",
    "s5 = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcflurry_s5.csv\"),\n",
    "    usecols=[\"hla\", \"peptide\"])\n",
    "print(\"loading S6...\")\n",
    "s6 = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcflurry_s6.csv\"),\n",
    "    usecols=[\"hla\", \"peptide\"])\n",
    "s3 = s3[[\"allele\", \"peptide\"]]\n",
    "s5 = s5[[\"hla\",    \"peptide\"]]\n",
    "s6 = s6[[\"hla\",    \"peptide\"]]\n",
    "s3.columns = [\"mhc\",\"pep\"]\n",
    "s5.columns = [\"mhc\",\"pep\"]\n",
    "s6.columns = [\"mhc\",\"pep\"]\n",
    "print(\"exploding S6 multiallelic peptides...\")\n",
    "s6.pep = s6.pep.apply(lambda x: x.split(' '))\n",
    "s6 = s6.explode(\"pep\")\n",
    "mhcflurryOther = pd.concat((s3,s5,s6))\n",
    "del s3, s5, s6\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1322c",
   "metadata": {},
   "source": [
    "#### Load PRIME-1.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49699a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PRIME neoepitopes...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load all PRIME-1.0 data into var: prime1\n",
    "print(\"loading PRIME neoepitopes...\")\n",
    "cols = [\"Allele\", \"Mutant\", \"Immunogenicity\", \"StudyOrigin\"]\n",
    "prime1 = pd.read_excel(\n",
    "    os.path.join(srcpath, \"prime1.xlsx\"),\n",
    "    skiprows=2,\n",
    "    usecols=cols)\n",
    "prime1 = prime1[cols]\n",
    "prime1[\"rand\"] = prime1.StudyOrigin.apply(lambda x: x == \"Random\")\n",
    "prime1.drop(\"StudyOrigin\", axis=1, inplace=True)\n",
    "prime1.columns = [\"mhc\", \"pep\", \"tgt\", \"rand\"]\n",
    "prime1.mhc = prime1.mhc.apply(lambda x: \"HLA-{}*{}:{}\".format(x[0], x[1:3], x[3:]))\n",
    "print(\"standardizing dataframe...\")\n",
    "prime1 = standardizeDF(prime1)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a265a",
   "metadata": {},
   "source": [
    "#### Load PRIME-2.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a87d0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading PRIME neoepitopes...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load PRIME-2.0 neoepitope data into var: prime2\n",
    "print(\"loading PRIME neoepitopes...\")\n",
    "cols = [\"Allele\", \"Mutant\", \"Immunogenicity\",\"Random\"]\n",
    "prime2 = pd.read_excel(\n",
    "    os.path.join(srcpath, \"prime2.xlsx\"),\n",
    "    skiprows=1,\n",
    "    usecols=cols)\n",
    "prime2 = prime2[cols]\n",
    "prime2.columns = [\"mhc\", \"pep\", \"tgt\", \"rand\"]\n",
    "prime2.mhc = prime2.mhc.apply(lambda x: \"HLA-{}*{}:{}\".format(x[0], x[1:3], x[3:]))\n",
    "print(\"standardizing dataframe...\")\n",
    "prime2 = standardizeDF(prime2)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2a1b92",
   "metadata": {},
   "source": [
    "#### Load MixMHCpred-2.2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06188bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MixMHCpred-2.2 data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"loading MixMHCpred-2.2 data...\")\n",
    "mixmhcpred = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mixmhcpred.txt\"),\n",
    "    delimiter='\\t',\n",
    "    skiprows=1)\n",
    "mixmhcpred = mixmhcpred[[\"Allele\", \"Peptide\"]]\n",
    "mixmhcpred.columns = [\"mhc\", \"pep\"]\n",
    "mixmhcpred.mhc = mixmhcpred.mhc.apply(lambda x: \"HLA-\" + x[0] + '*' + x[1:3] + ':' + x[-2:])\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56559814",
   "metadata": {},
   "source": [
    "#### Load MHCnuggets-2.4.0 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a370f80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading mhcnuggets...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"reading mhcnuggets...\")\n",
    "cols = [\"mhc\", \"peptide\"]\n",
    "mhcnuggets = pd.read_csv(\n",
    "    os.path.join(srcpath, \"mhcnuggets.csv\"),\n",
    "    usecols=cols)\n",
    "mhcnuggets = mhcnuggets[[\"mhc\", \"peptide\"]]\n",
    "mhcnuggets.columns = [\"mhc\",\"pep\"]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b8dbb7",
   "metadata": {},
   "source": [
    "#### Load TransPHLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13ddb667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading TransPHLA data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"loading TransPHLA data...\")\n",
    "cols = [\"HLA\",\"peptide\"]\n",
    "transphla  = pd.read_csv(\n",
    "    os.path.join(srcpath, \"transphla.csv\"),\n",
    "    usecols=cols)\n",
    "transphla = transphla[cols].copy()\n",
    "transphla.columns = [\"mhc\", \"pep\"]\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554c15b2",
   "metadata": {},
   "source": [
    "#### Load HLAthena data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a89e5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading HLAthena...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"loading HLAthena...\")\n",
    "hlathenaExcel = pd.ExcelFile(os.path.join(srcpath, \"hlathena.xlsx\"))\n",
    "hlathena = list()\n",
    "for x in hlathenaExcel.sheet_names[1:]:\n",
    "    hlathena.append(hlathenaExcel.parse(x, usecols=[\"sequence\"]))\n",
    "    hlathena[-1][\"mhc\"] = \"HLA-\" + x[0] + '*' + x[1:3] + ':' + x[-2:]\n",
    "hlathena = pd.concat(hlathena)\n",
    "hlathena = hlathena[[\"mhc\", \"sequence\"]]\n",
    "hlathena.columns = [\"mhc\", \"pep\"]\n",
    "del hlathenaExcel\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beb9472",
   "metadata": {},
   "source": [
    "#### Load NEPdb data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44787db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NEPdb data...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load NEPdb HLA-A/B/C data into var: nepdb\n",
    "print(\"loading NEPdb data...\")\n",
    "cols = [\"alleleA\", \"mut_peptide\", \"response\"]\n",
    "nepdb = pd.concat(\n",
    "    [pd.read_csv(\n",
    "        os.path.join(srcpath, \"nepdb_{}.csv\".format(x)),\n",
    "        usecols=cols)\n",
    "    for x in ['a','b','c']])\n",
    "nepdb = nepdb[cols]\n",
    "nepdb.columns = [\"mhc\", \"pep\", \"tgt\"]\n",
    "nepdb.mhc = nepdb.mhc.apply(lambda x: \"HLA-\" + x)\n",
    "nepdb.tgt = nepdb.tgt.apply(lambda x: int(x.lower()=='p'))\n",
    "print(\"standardizing dataframe...\")\n",
    "nepdb = standardizeDF(nepdb)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e92890",
   "metadata": {},
   "source": [
    "#### Load Neopepsee data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21992e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Neopepsee data...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load Neopepsee data into var: neopepsee\n",
    "print(\"loading Neopepsee data...\")\n",
    "sciencecols = [\"IEDB_A_type\", \"WT_AA\", \"Answer\"]\n",
    "bloodcols = [\"HLA-A_allele\", \"WT_AA\", \"Answer\"]\n",
    "science = pd.read_excel(\n",
    "    os.path.join(srcpath, \"neopepsee.xlsx\"),\n",
    "    sheet_name=\"Science\",\n",
    "    usecols=sciencecols)\n",
    "blood = pd.read_excel(\n",
    "    os.path.join(srcpath, \"neopepsee.xlsx\"),\n",
    "    sheet_name=\"Blood\",\n",
    "    usecols=bloodcols)\n",
    "science = science[sciencecols]\n",
    "blood = blood[bloodcols]\n",
    "science.columns = [\"mhc\",\"pep\",\"tgt\"]\n",
    "blood.columns = [\"mhc\",\"pep\",\"tgt\"]\n",
    "science.tgt = science.tgt.apply(lambda x: int(x))\n",
    "blood.mhc = blood.mhc.apply(lambda x: \"HLA-\" + x)\n",
    "blood.tgt = blood.tgt.apply(lambda x: int(x.strip()[0].lower() == \"p\"))\n",
    "neopepsee = pd.concat((science, blood))\n",
    "del science, blood\n",
    "print(\"standardizing dataframe...\")\n",
    "neopepsee = standardizeDF(neopepsee)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1d7a8",
   "metadata": {},
   "source": [
    "#### Load TESLA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce1241a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading TESLA data...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load TESLA S4 and S7 data into var: tesla\n",
    "print(\"loading TESLA data...\")\n",
    "cols = [\"PMHC\", \"VALIDATED\"]\n",
    "s4 = pd.read_excel(\n",
    "    os.path.join(srcpath, \"tesla_s4.xlsx\"),\n",
    "    usecols=cols)\n",
    "s7 = pd.read_excel(\n",
    "    os.path.join(srcpath, \"tesla_s7.xlsx\"),\n",
    "    usecols=cols)\n",
    "s4.columns = [\"pmhc\", \"tgt\"]\n",
    "s7.columns = [\"pmhc\", \"tgt\"]\n",
    "s4[\"mhc\"] = s4.pmhc.apply(lambda x: \"HLA-\" + x[:x.index('_')])\n",
    "s7[\"mhc\"] = s7.pmhc.apply(lambda x: \"HLA-\" + x[:x.index('_')])\n",
    "s4[\"pep\"] = s4.pmhc.apply(lambda x: x[x.index('_')+1:])\n",
    "s7[\"pep\"] = s7.pmhc.apply(lambda x: x[x.index('_')+1:])\n",
    "s4 = s4[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "s7 = s7[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "tesla = pd.concat((s4, s7))\n",
    "del s4, s7\n",
    "print(\"standardizing dataframe...\")\n",
    "tesla = standardizeDF(tesla)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1c61e",
   "metadata": {},
   "source": [
    "#### Load IEDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df3bf95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading IEDB data...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load IEDB data into var: iedb\n",
    "print(\"loading IEDB data...\")\n",
    "iedb = pd.read_csv(\n",
    "    os.path.join(srcpath, \"iedb.csv\"),\n",
    "    skiprows=1,\n",
    "    usecols=[11,87,101])\n",
    "iedb.columns = [\"pep\",\"tgt\",\"mhc\"]\n",
    "iedb = iedb[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (' ' not in x) and ('-' in x) and ('*' in x) and (':' in x))]\n",
    "# NetMHCpan does not have a pseudosequence for the below alleles\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (x != \"HLA-B*44:01\"))]\n",
    "# MixMHCpred cannot make predictions for the below alleles\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (x != \"HLA-A*02:09\"))]\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (x != \"HLA-B*07:06\"))]\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (x != \"HLA-B*40:10\"))]\n",
    "# HLAthena cannot make predictions for the below alleles\n",
    "iedb = iedb[iedb.mhc.apply(lambda x: (x != \"HLA-A*02:16\"))]\n",
    "iedb.tgt = iedb.tgt.apply(lambda x: int(x.strip()[0].lower() == \"p\"))\n",
    "print(\"standardizing dataframe...\")\n",
    "iedb = standardizeDF(iedb)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f39249",
   "metadata": {},
   "source": [
    "#### Load MANAFEST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d2d9dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading MANAFEST data...\n",
      "standardizing dataframe...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load MANAFEST data into var: manafest\n",
    "print(\"loading MANAFEST data...\")\n",
    "manafest = pd.read_csv(os.path.join(srcpath, \"manafest.csv\"))\n",
    "manafest = manafest[[\"mhc\",\"pep\",\"tgt\"]]\n",
    "print(\"standardizing dataframe...\")\n",
    "manafest = standardizeDF(manafest)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae472d84",
   "metadata": {},
   "source": [
    "#### Make training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "048160a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making el_trainval...\n",
      "making im_trainval...\n",
      "making im_test...\n",
      "removing el_test from el_trainval...\n",
      "removing all EL data and prime from immuno test data...\n",
      "removing long peptides from immuno test data...\n",
      "removing all other training data from immuno test data...\n",
      "splitting el_trainval and im_trainval into training and validation...\n",
      "making complete EL dataset...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def rmintersection(df, *others):\n",
    "    for other in others:\n",
    "        df = df[~df.index.isin(other.index)].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"making el_trainval...\")\n",
    "eltrainval = dedup(pd.concat((netmhcpanTrain, mhcflurryTrain))).sort_index()\n",
    "eltrainval = filterlen(eltrainval, maxlen=14)\n",
    "\n",
    "print(\"making im_trainval...\")\n",
    "prime = dedup(pd.concat((prime1, prime2))).sort_index()\n",
    "imtrainval = prime[prime.rand == 0].copy()\n",
    "imtrainval = filterlen(imtrainval, minlen=9, maxlen=10)\n",
    "\n",
    "print(\"making im_test...\")\n",
    "imtest = dedup(pd.concat((manafest, tesla, nepdb, neopepsee))).sort_index()\n",
    "\n",
    "print(\"removing el_test from el_trainval...\")\n",
    "eltrainval = rmintersection(eltrainval, netmhcpanTest)\n",
    "\n",
    "print(\"removing all EL data and prime from immuno test data...\")\n",
    "imtest = rmintersection(imtest, eltrainval, netmhcpanTest, prime)\n",
    "iedbTest = rmintersection(iedb.copy(), eltrainval, netmhcpanTest, prime)\n",
    "\n",
    "print(\"removing long peptides from immuno test data...\")\n",
    "# HLAthena can only make predictions on peptides of length [8,11]\n",
    "imtest = filterlen(imtest, minlen=8, maxlen=11)\n",
    "iedbTest = filterlen(iedbTest, minlen=8, maxlen=11)\n",
    "\n",
    "print(\"removing all other training data from immuno test data...\")\n",
    "otherTrain = uid(pd.concat((mhcflurryOther, mixmhcpred, mhcnuggets, transphla, hlathena)))\n",
    "imtest = rmintersection(imtest, otherTrain)\n",
    "iedbTest = rmintersection(iedbTest, otherTrain)\n",
    "\n",
    "print(\"splitting el_trainval and im_trainval into training and validation...\")\n",
    "\n",
    "elval = eltrainval.iloc[::10].copy()\n",
    "eltrain = eltrainval[~eltrainval.index.isin(elval.index)]\n",
    "\n",
    "imval = imtrainval.iloc[::10].copy()\n",
    "imtrain = imtrainval[~imtrainval.index.isin(imval.index)]\n",
    "\n",
    "print(\"making complete EL dataset...\")\n",
    "\n",
    "el = pd.concat((eltrainval, netmhcpanTest)).sort_index()\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03361f9",
   "metadata": {},
   "source": [
    "#### Write training and evaluation datasets to output dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69ba33cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing el_train...\n",
      "writing el_val...\n",
      "writing el_trainval...\n",
      "writing el_test...\n",
      "writing el...\n",
      "writing im_train...\n",
      "writing im_val...\n",
      "writing im_trainval...\n",
      "writing im_test...\n",
      "writing iedb...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(outpath):\n",
    "    os.makedirs(outpath)\n",
    "\n",
    "print(\"writing el_train...\")\n",
    "eltrain.to_csv(os.path.join(outpath, \"el_train.csv\"), index=False)\n",
    "print(\"writing el_val...\")\n",
    "elval.to_csv(os.path.join(outpath, \"el_val.csv\"), index=False)\n",
    "print(\"writing el_trainval...\")\n",
    "eltrainval.to_csv(os.path.join(outpath, \"el_trainval.csv\"), index=False)\n",
    "print(\"writing el_test...\")\n",
    "netmhcpanTest.to_csv(os.path.join(outpath, \"el_test.csv\"), index=False)\n",
    "print(\"writing el...\")\n",
    "el.to_csv(os.path.join(outpath, \"el.csv\"), index=False)\n",
    "\n",
    "print(\"writing im_train...\")\n",
    "imtrain.to_csv(os.path.join(outpath, \"im_train.csv\"), index=False)\n",
    "print(\"writing im_val...\")\n",
    "imval.to_csv(os.path.join(outpath, \"im_val.csv\"), index=False)\n",
    "print(\"writing im_trainval...\")\n",
    "imtrainval.to_csv(os.path.join(outpath, \"im_trainval.csv\"), index=False)\n",
    "print(\"writing im_test...\")\n",
    "imtest.to_csv(os.path.join(outpath, \"im_test.csv\"), index=False)\n",
    "\n",
    "print(\"writing iedb...\")\n",
    "iedbTest.to_csv(os.path.join(outpath, \"iedb.csv\"), index=False)\n",
    "\n",
    "\n",
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
