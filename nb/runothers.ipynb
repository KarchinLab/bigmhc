{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec730cd0",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------\n",
    "\n",
    "Copyright 2022 Benjamin Alexander Albert \\[Karchin Lab\\]\n",
    "\n",
    "All Rights Reserved\n",
    "\n",
    "BigMHC Academic License\n",
    "\n",
    "runothers.ipynb\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "#### Run Other Methods for Comparison Against BigMHC\n",
    "\n",
    "Create a dir, which we will call `thirdparty`\n",
    " * Store all of the below downloadables will be placed in `thirdparty`\n",
    " * All results will be placed in a dir called `out` within the `thirdparty`\n",
    "\n",
    "Install **NetMHCpan-4.1** https://services.healthtech.dtu.dk/service.php?NetMHCpan-4.1\n",
    "  * Download NetMHCpan-4.1b and extract all contents to a dir called netmhcpan\n",
    "    * the directory structure should be `thirdparty`/netmhcpan/ where netmhcpan contains:\n",
    "      * Linux_x86_64 dir\n",
    "      * netMHCpan tcsh script\n",
    "      * netMHCpan.1 file\n",
    "      * netMHCpan-4.1.readme\n",
    "      * test dir\n",
    "  * Follow the instructions of the netmhcpan-4.1.readme to install and test NetMHCpan-4.1\n",
    "    * The instructions are summarized below for completeness:\n",
    "      * `wget https://services.healthtech.dtu.dk/services/NetMHCpan-4.1/data.tar.gz`\n",
    "      * `tar -xvf data.tar.gz`\n",
    "      * `rm data.tar.gz`\n",
    "      * Edit the `netMHCpan` script and replace the default NMHOME var with the full path to the `netmhcpan` dir\n",
    "      * From within the `netmhcpan`/test dir, run the following:\n",
    "        * ../netMHCpan -p test.pep > test.pep.myout\n",
    "        * ../netMHCpan test.fsa > test.fsa.myout\n",
    "        * ../netMHCpan -hlaseq B0702.fsa -p test.pep > test.pep_userMHC.myout\n",
    "        * ../netMHCpan -p test.pep -BA -xls -a HLA-A01:01,HLA-A02:01 -xlsfile NetMHCpan_myout.xls\n",
    "      * Then diff each of the `.myout` files with their respective `.out` files:\n",
    "        * diff test.pep.out test.pep.myout\n",
    "        * diff test.fsa.out test.fsa.myout\n",
    "        * diff test.pep_userMHC.out test.pep_userMHC.myout\n",
    "        * diff NetMHCpan_out.xls NetMHCpan_myout.xls\n",
    "\n",
    "Install **MHCflurry-2.0** https://github.com/openvax/mhcflurry\n",
    "  * `conda install tensorflow` or `pip install tensorflow` version 2.2.0 or later\n",
    "  * `pip install mhcflurry`\n",
    "  * `mhcflurry-downloads fetch`\n",
    "\n",
    "Install **MHCnuggets** https://github.com/KarchinLab/mhcnuggets\n",
    "  * `git clone https://github.com/KarchinLab/mhcnuggets.git`\n",
    "\n",
    "Install **TransPHLA** https://github.com/a96123155/TransPHLA-AOMP\n",
    "  * `git clone https://github.com/a96123155/TransPHLA-AOMP.git`\n",
    "  * We need to remove the sigmoidal activation to prevent output values from being squashed to 0 or 1\n",
    "    * In the `model.py` file found in `TransPHLA-AOMP/TransPHLA-AOMP`, within the `eval_step` function, do the following:\n",
    "      * Replace: `y_prob_val = nn.Softmax(dim = 1)(val_outputs)[:, 1].cpu().detach().numpy()`\n",
    "      * With: `y_prob_val = val_outputs[:, 1].cpu().detach().numpy()`\n",
    "  * On line 99 of TransPHLA-AOMP/TransPHLA-AOMP/pHLAIformer.py, there is an erroneous indentation\n",
    "    * Remove a single tab in front of `log = Logger(errLogPath)`\n",
    "  * On lines 63-65 of TransPHLA-AOMP/TransPHLA-AOMP/pHLAIformer.py, change the argument type from `bool` to `int`\n",
    "    * As of November 27, 2022, Argparse does not support setting argument type `bool`\n",
    "  * To enable CUDA, set `use_cuda = True` on each of the following lines:\n",
    "    * line 160 of TransPHLA-AOMP/TransPHLA-AOMP/pHLAIformer.py\n",
    "    * line 62 of TransPHLA-AOMP/TransPHLA-AOMP/model.py\n",
    "\n",
    "Install **MixMHCpred2.1** and **MixMHCpred2.2** https://github.com/GfellerLab/MixMHCpred/releases\n",
    "  * Download and extract MixMHCpred v2.1 and v2.2\n",
    "    * `wget https://github.com/GfellerLab/MixMHCpred/archive/refs/tags/v2.1.tar.gz`\n",
    "    * `wget https://github.com/GfellerLab/MixMHCpred/archive/refs/tags/v2.2.tar.gz`\n",
    "    * `tar -xzvf v2.1.tar.gz`\n",
    "    * `tar -xzvf v2.2.tar.gz`\n",
    "    * `rm v2.1.tar.gz v2.1.tar.gz`\n",
    "  * Compile the models\n",
    "    * `g++ -O3 MixMHCpred-2.1/lib/MixMHCpred.cc -o MixMHCpred-2.1/lib/MixMHCpred.x`\n",
    "    * `g++ -O3 MixMHCpred-2.2/lib/MixMHCpred.cc -o MixMHCpred-2.2/lib/MixMHCpred.x`\n",
    "  * Edit the `MixMHCpred` scripts and set the lib_path var to the full path of MixMHCpred-2.x/lib\n",
    "  * Test the installation from within each of the MixMHCpred-2.x dirs:\n",
    "    * `./MixMHCpred -i test/test.fa -o test/out.txt -a A0101,A2501,B0801,B1801`\n",
    "    * `diff test/out_compare.txt test/out.txt`\n",
    "\n",
    "Install **PRIME-1.0** and **PRIME-2.0** https://github.com/GfellerLab/PRIME/releases\n",
    "  * Download and extract PRIME v1.0 and v2.0:\n",
    "    * `wget https://github.com/GfellerLab/PRIME/archive/refs/tags/v1.0.tar.gz`\n",
    "    * `wget https://github.com/GfellerLab/PRIME/archive/refs/tags/v2.0.tar.gz`\n",
    "    * `tar -xzvf v1.0.tar.gz`\n",
    "    * `tar -xzvf v2.0.tar.gz`\n",
    "    * `rm v1.0.tar.gz v2.0.tar.gz`\n",
    "  * Compile PRIME-2.0 (version 1.0 does not need compilation)\n",
    "    * `g++ -O3 PRIME-2.0/lib/PRIME.cc -o PRIME-2.0/lib/PRIME.x`\n",
    "  * Edit the `PRIME` scripts and set the lib_path var to the full path of PRIME-1.x/lib\n",
    "  * Test the installation from within each of the PRIME-1.x dirs\n",
    "    * Test PRIME-1.0\n",
    "      * `./PRIME -i test/test.txt -o test/out.txt -a A0201,A0101 -mix MixMHCpred2.1_path`\n",
    "      * `diff test/out_compare.txt test/out.txt`\n",
    "    * Test PRIME-2.0\n",
    "      * `./PRIME -i test/test.txt -o test/out.txt -a A0101,A2501,B0801,B1801 -mix MixMHCpred2.2_path`\n",
    "      * `diff test/out_compare.txt test/out.txt`\n",
    "  \n",
    "------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d2fdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datadir = os.path.abspath(\"../data\")\n",
    "outdir = os.path.join(datadir, \"out\")\n",
    "prddir = os.path.join(datadir, \"prd\")\n",
    "\n",
    "useranks = True\n",
    "\n",
    "# HLAthena does not appear to like tmpdir being \"/tmp\"\n",
    "tmpdir = os.path.abspath(os.path.join(datadir, \"tmp\"))\n",
    "tmpfile = os.path.abspath(os.path.join(tmpdir, \"tmp.csv\"))\n",
    "\n",
    "thirdparty = os.path.abspath(\"../third_party\")\n",
    "pseudofile = os.path.join(thirdparty, \"netmhcpan/data/MHC_pseudo.dat\")\n",
    "\n",
    "gid = os.popen(\"id -g $USER\").read()\n",
    "\n",
    "if not os.path.exists(tmpdir):\n",
    "    os.makedirs(tmpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31705421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def uid(df):\n",
    "    df[\"uid\"] = df[\"mhc\"] + '_' + df[\"pep\"]\n",
    "    return df.set_index(\"uid\", drop=True)\n",
    "\n",
    "\n",
    "def subprocrun(cmd, cwd=None, pipe=False):\n",
    "    if pipe:\n",
    "        res = subprocess.run(\n",
    "            cmd.split(),\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "            cwd=cwd)\n",
    "    else:\n",
    "        res = subprocess.run(\n",
    "            cmd.split(),\n",
    "            universal_newlines=True,\n",
    "            cwd=cwd)\n",
    "    return res.stdout\n",
    "\n",
    "\n",
    "def runmodel(func, data, name):\n",
    "    out = list()\n",
    "    for mhc, grp in data.groupby(\"mhc\"):\n",
    "        out.append(pd.Series(\n",
    "            data=func(mhc, grp[\"pep\"]),\n",
    "            index=grp.index,\n",
    "            name=name))\n",
    "        try:\n",
    "            os.remove(tmpfile)\n",
    "        except OSError:\n",
    "            pass\n",
    "    return pd.concat(out)\n",
    "\n",
    "\n",
    "def mhcnuggets(data, outfile):\n",
    "\n",
    "    def _run(mhc, pep):\n",
    "        pep.to_csv(\n",
    "            tmpfile,\n",
    "            header=False,\n",
    "            index=False)\n",
    "        cmd = \"python predict.py\" + \\\n",
    "            \" --class=I\" + \\\n",
    "            \" --peptides={}\".format(tmpfile) + \\\n",
    "            \" --allele={}\".format(mhc.replace('*','')) + \\\n",
    "            \" --output={}\".format(outfile)\n",
    "        subprocrun(\n",
    "            cmd=cmd,\n",
    "            cwd=os.path.join(thirdparty, \"mhcnuggets/mhcnuggets/src\"))\n",
    "        ic50 = pd.read_csv(\n",
    "            outfile,\n",
    "            usecols=[\"ic50\"]).iloc[:,0].tolist()\n",
    "        return 1 - (np.log(ic50) / np.log(50000))\n",
    "\n",
    "    return runmodel(\n",
    "        func=_run,\n",
    "        data=data,\n",
    "        name=\"MHCnuggets-2.4.0\")\n",
    "\n",
    "\n",
    "def netmhcpan(data, outfile):\n",
    "\n",
    "    def _run(mhc, pep):\n",
    "        pep.to_csv(\n",
    "            tmpfile,\n",
    "            header=False,\n",
    "            index=False)\n",
    "        cmd = \"./netMHCpan\" + \\\n",
    "            \" -p {}\".format(tmpfile) + \\\n",
    "            \" -a {}\".format(mhc.replace('*','')) + \\\n",
    "            \" -xls\" + \\\n",
    "            \" -xlsfile {}\".format(outfile)\n",
    "        subprocrun(\n",
    "            cmd=cmd,\n",
    "            cwd=os.path.join(thirdparty, \"netmhcpan\"))\n",
    "        return pd.read_csv(\n",
    "            outfile,\n",
    "            delimiter='\\t',\n",
    "            skiprows=1,\n",
    "            usecols=[\"EL_Rank\" if useranks else \"EL-score\"]).iloc[:,0].tolist()\n",
    "\n",
    "    return runmodel(\n",
    "        func=_run,\n",
    "        data=data,\n",
    "        name=\"NetMHCpan-4.1\")\n",
    "\n",
    "\n",
    "def hlathena(data, outfile):\n",
    "    def _run(mhc, pep):\n",
    "        pep.to_csv(\n",
    "            tmpfile,\n",
    "            index=False)\n",
    "        cmd = \"docker run --user 0:{} \".format(gid) + \\\n",
    "            \" -v {}:{}\".format(tmpdir, tmpdir) + \\\n",
    "            \" -w {}\".format(tmpdir) + \\\n",
    "            \" ssarkizova/hlathena-external predict\" + \\\n",
    "            \" --runID hlathena\" + \\\n",
    "            \" --rundir {}\".format(tmpdir) + \\\n",
    "            \" -p {}\".format(tmpfile) + \\\n",
    "            \" -a {}\".format(mhc[4:].replace('*','').replace(':',''))\n",
    "        subprocrun(\n",
    "            cmd=cmd,\n",
    "            cwd=tmpdir)\n",
    "        prd = pd.read_csv(\n",
    "            os.path.join(tmpdir, \"hlathena-predictions.txt\"),\n",
    "            delimiter='\\t',\n",
    "            usecols=[0,4 if useranks else 3])\n",
    "        return prd.set_index(\"pep\").loc[pep].iloc[:,0].tolist()\n",
    "\n",
    "    return runmodel(\n",
    "        func=_run,\n",
    "        data=data,\n",
    "        name=\"HLAthena\")\n",
    "\n",
    "\n",
    "def _gfeller(method, data, outfile):\n",
    "\n",
    "    def _run(mhc, pep):\n",
    "        pep.to_csv(\n",
    "            tmpfile,\n",
    "            header=False,\n",
    "            index=False)\n",
    "        if method==\"PRIME-1.0\":\n",
    "            cmd = \"./PRIME -mix ../MixMHCpred-2.1/MixMHCpred\"\n",
    "            cwd = os.path.join(thirdparty, \"PRIME-1.0\")\n",
    "        elif method==\"PRIME-2.0\":\n",
    "            cmd = \"./PRIME -mix ../MixMHCpred-2.2/MixMHCpred\"\n",
    "            cwd = os.path.join(thirdparty, \"PRIME-2.0\")\n",
    "        elif method==\"MixMHCpred-2.1\":\n",
    "            cmd = \"./MixMHCpred\"\n",
    "            cwd = os.path.join(thirdparty, \"MixMHCpred-2.1\")\n",
    "        elif method==\"MixMHCpred-2.2\":\n",
    "            cmd = \"./MixMHCpred\"\n",
    "            cwd = os.path.join(thirdparty, \"MixMHCpred-2.2\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unexpected gfeller method: {}\".format(method))\n",
    "        cmd += \\\n",
    "            \" -i {}\".format(tmpfile) + \\\n",
    "            \" -a {}\".format(mhc) + \\\n",
    "            \" -o {}\".format(outfile)\n",
    "        subprocrun(\n",
    "            cmd=cmd,\n",
    "            cwd=cwd)\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                outfile,\n",
    "                delimiter='\\t',\n",
    "                skiprows=11,\n",
    "                usecols=[\"%Rank_bestAllele\" if useranks else \"Score_bestAllele\"]).iloc[:,0].tolist()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return [float(\"nan\") for _ in range(len(pep))]\n",
    "\n",
    "    return runmodel(\n",
    "        func=_run,\n",
    "        data=data,\n",
    "        name=method)\n",
    "\n",
    "\n",
    "def prime1(data, outfile):\n",
    "    return _gfeller(\"PRIME-1.0\", data, outfile)\n",
    "\n",
    "\n",
    "def prime2(data, outfile):\n",
    "    return _gfeller(\"PRIME-2.0\", data, outfile)\n",
    "\n",
    "\n",
    "def mixmhcpred21(data, outfile):\n",
    "    return _gfeller(\"MixMHCpred-2.1\", data, outfile)\n",
    "\n",
    "\n",
    "def mixmhcpred22(data, outfile):\n",
    "    return _gfeller(\"MixMHCpred-2.2\", data, outfile)\n",
    "\n",
    "\n",
    "def mhcflurry(data, outfile):\n",
    "    data.to_csv(\n",
    "        tmpfile,\n",
    "        columns=[\"mhc\",\"pep\"],\n",
    "        header=[\"allele\",\"peptide\"],\n",
    "        index=False)\n",
    "    cmd = \"mhcflurry-predict {} --out={}\".format(\n",
    "        tmpfile, outfile)\n",
    "    subprocrun(cmd=cmd)\n",
    "\n",
    "    out = pd.read_csv(\n",
    "        outfile,\n",
    "        usecols=[\"mhcflurry_presentation_percentile\" if useranks else \"mhcflurry_presentation_score\"]\n",
    "    ).iloc[:,0].tolist()\n",
    "\n",
    "    name = \"MHCflurry-2.0\"\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mhc\":data[\"mhc\"],\n",
    "        \"pep\":data[\"pep\"],\n",
    "        name:out})\n",
    "\n",
    "    return uid(out)[name]\n",
    "\n",
    "\n",
    "def transphla(data, outfile):\n",
    "\n",
    "    chunksize = 100*1000\n",
    "\n",
    "    mhcmap = dict()\n",
    "    with open(pseudofile, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            mhc = line[:line.find(' ')]\n",
    "            seq = line[line.rfind(' ')+1:]\n",
    "            mhcmap[mhc] = seq\n",
    "\n",
    "    mhclines = [\n",
    "        \">{}\\n{}\".format(\n",
    "            mhc,\n",
    "            mhcmap[mhc.replace('*','')])\n",
    "        for mhc in data[\"mhc\"]]\n",
    "\n",
    "    peplines = [\n",
    "        \">{}\\n{}\".format(\n",
    "            pep,\n",
    "            pep)\n",
    "        for pep in data[\"pep\"]]\n",
    "\n",
    "    col = \"y_prob\"\n",
    "    idx1 = 0\n",
    "    preds = list()\n",
    "    while idx1 < len(data):\n",
    "        idx2 = min(idx1+chunksize, len(data))\n",
    "        with open(outfile, 'w') as f:\n",
    "            f.write('\\n'.join(mhclines[idx1:idx2]))\n",
    "        with open(tmpfile, 'w') as f:\n",
    "            f.write('\\n'.join(peplines[idx1:idx2]))\n",
    "        cmd = \"python pHLAIformer.py\" + \\\n",
    "            \" --peptide_file={}\".format(tmpfile) + \\\n",
    "            \" --HLA_file={}\".format(outfile) + \\\n",
    "            \" --cut_length=15\" + \\\n",
    "            \" --output_dir=/tmp\" + \\\n",
    "            \" --output_attention=0\" + \\\n",
    "            \" --output_heatmap=0\" + \\\n",
    "            \" --output_mutation=0\"\n",
    "        subprocrun(\n",
    "            cmd=cmd,\n",
    "            cwd=os.path.join(thirdparty, \"TransPHLA-AOMP/TransPHLA-AOMP\"))\n",
    "        preds.append(\n",
    "            pd.read_csv(\n",
    "                \"/tmp/predict_results.csv\",\n",
    "                usecols=[col]))\n",
    "        idx1 = idx2\n",
    "\n",
    "    out = pd.concat(preds)[col].tolist()\n",
    "\n",
    "    name = \"TransPHLA\"\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mhc\":data[\"mhc\"],\n",
    "        \"pep\":data[\"pep\"],\n",
    "        name:out})\n",
    "\n",
    "    return uid(out)[name]\n",
    "\n",
    "\n",
    "def run(models, filename):\n",
    "    print(\"running {}...\".format(filename[:filename.index('.')]))\n",
    "    prdfile = os.path.join(prddir, filename)\n",
    "    df = uid(pd.read_csv(os.path.join(outdir, filename)))\n",
    "    for m in models:\n",
    "        start = time.time()\n",
    "        out = m(df, prdfile)\n",
    "        print(m.__name__, time.time() - start)\n",
    "        df = pd.concat((df, out), axis=1)\n",
    "    df.to_csv(prdfile, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16f7e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = run([netmhcpan, mhcnuggets, mixmhcpred21, mixmhcpred22, transphla],\n",
    "    \"el_test.csv\")\n",
    "\n",
    "df = run([netmhcpan, mhcflurry, mhcnuggets, mixmhcpred21, mixmhcpred22, prime1, prime2, transphla, hlathena],\n",
    "    \"asdf.csv\")\n",
    "\n",
    "df = run([netmhcpan, mhcflurry, mhcnuggets, mixmhcpred21, mixmhcpred22, prime1, prime2, transphla, hlathena],\n",
    "    \"iedb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15fa8d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running asdf...\n",
      "# /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/bin/netMHCpan -p /home/benji/bigmhc_rev/data/tmp/tmp.csv -a HLA-A01:01 -xls -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv\n",
      "# Wed Jan 18 14:21:23 2023\n",
      "# User: benji\n",
      "# PWD : /home/benji/bigmhc_rev/third_party/netmhcpan\n",
      "# Host: Linux orthrus1 5.10.0-19-amd64 x86_64\n",
      "# -p       1                    Use peptide input\n",
      "# -a       HLA-A01:01           MHC allele\n",
      "# -xls     1                    Save output to xls file\n",
      "# -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "# Command line parameters set to:\n",
      "#\t[-rdir filename]     /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64 Home directory for NetMHpan\n",
      "#\t[-syn filename]      /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/synlist.bin Synaps file\n",
      "#\t[-v]                 0                    Verbose mode\n",
      "#\t[-dirty]             0                    Dirty mode, leave tmp dir+files\n",
      "#\t[-tdir filename]     /tmp/netMHCpanXXXXXX Temporary directory (made with mkdtemp)\n",
      "#\t[-hlapseudo filename] /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/MHC_pseudo.dat File with MHC pseudo sequences\n",
      "#\t[-hlaseq filename]                        File with full length MHC sequences\n",
      "#\t[-a line]            HLA-A01:01           MHC allele\n",
      "#\t[-f filename]                             File name with input\n",
      "#\t[-w]                 0                    w option for webface\n",
      "#\t[-s]                 0                    Sort output on descending affinity\n",
      "#\t[-p]                 1                    Use peptide input\n",
      "#\t[-rth float]         0.500000             Rank Threshold for high binding peptides\n",
      "#\t[-rlt float]         2.000000             Rank Threshold for low binding peptides\n",
      "#\t[-l string]          8,9,10,11            Peptide length [8-11] (multiple length with ,)\n",
      "#\t[-xls]               1                    Save output to xls file\n",
      "#\t[-xlsfile filename]  /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "#\t[-t float]           -99.900002           Threshold for output (%rank) [<0 print all]\n",
      "#\t[-thrfmt filename]   /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/threshold/%s.thr.%s Format for threshold filenames\n",
      "#\t[-expfix]            0                    Exclude prefix from synlist\n",
      "#\t[-version filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/version File with version information\n",
      "#\t[-inptype int]       0                    Input type [0] FASTA [1] Peptide\n",
      "#\t[-listMHC]           0                    Print list of alleles included in netMHCpan\n",
      "#\t[-allname filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/allelenames File with print names for alleles\n",
      "#\t[-BA]                0                    Include Binding affinity prediction\n",
      "\n",
      "# NetMHCpan version 4.1b\n",
      "\n",
      "# Tmpdir made /tmp/netMHCpanBfE27d\n",
      "# Input is in PEPTIDE format\n",
      "\n",
      "# Make EL predictions\n",
      "\n",
      "HLA-A01:01 : Distance to training data  0.000 (using nearest neighbor HLA-A01:01)\n",
      "\n",
      "# Rank Threshold for Strong binding peptides   0.500\n",
      "# Rank Threshold for Weak binding peptides   2.000\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      " Pos         MHC        Peptide      Core Of Gp Gl Ip Il        Icore        Identity  Score_EL %Rank_EL BindLevel\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "   1 HLA-A*01:01    ALDKLSSQHLY ALDSSQHLY  0  3  2  0  0  ALDKLSSQHLY         PEPLIST 0.9374210    0.036 <= SB\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Protein PEPLIST. Allele HLA-A*01:01. Number of high binders 1. Number of weak binders 0. Number of peptides 1\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "# /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/bin/netMHCpan -p /home/benji/bigmhc_rev/data/tmp/tmp.csv -a HLA-C16:01 -xls -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv\n",
      "# Wed Jan 18 14:21:23 2023\n",
      "# User: benji\n",
      "# PWD : /home/benji/bigmhc_rev/third_party/netmhcpan\n",
      "# Host: Linux orthrus1 5.10.0-19-amd64 x86_64\n",
      "# -p       1                    Use peptide input\n",
      "# -a       HLA-C16:01           MHC allele\n",
      "# -xls     1                    Save output to xls file\n",
      "# -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "# Command line parameters set to:\n",
      "#\t[-rdir filename]     /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64 Home directory for NetMHpan\n",
      "#\t[-syn filename]      /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/synlist.bin Synaps file\n",
      "#\t[-v]                 0                    Verbose mode\n",
      "#\t[-dirty]             0                    Dirty mode, leave tmp dir+files\n",
      "#\t[-tdir filename]     /tmp/netMHCpanXXXXXX Temporary directory (made with mkdtemp)\n",
      "#\t[-hlapseudo filename] /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/MHC_pseudo.dat File with MHC pseudo sequences\n",
      "#\t[-hlaseq filename]                        File with full length MHC sequences\n",
      "#\t[-a line]            HLA-C16:01           MHC allele\n",
      "#\t[-f filename]                             File name with input\n",
      "#\t[-w]                 0                    w option for webface\n",
      "#\t[-s]                 0                    Sort output on descending affinity\n",
      "#\t[-p]                 1                    Use peptide input\n",
      "#\t[-rth float]         0.500000             Rank Threshold for high binding peptides\n",
      "#\t[-rlt float]         2.000000             Rank Threshold for low binding peptides\n",
      "#\t[-l string]          8,9,10,11            Peptide length [8-11] (multiple length with ,)\n",
      "#\t[-xls]               1                    Save output to xls file\n",
      "#\t[-xlsfile filename]  /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "#\t[-t float]           -99.900002           Threshold for output (%rank) [<0 print all]\n",
      "#\t[-thrfmt filename]   /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/threshold/%s.thr.%s Format for threshold filenames\n",
      "#\t[-expfix]            0                    Exclude prefix from synlist\n",
      "#\t[-version filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/version File with version information\n",
      "#\t[-inptype int]       0                    Input type [0] FASTA [1] Peptide\n",
      "#\t[-listMHC]           0                    Print list of alleles included in netMHCpan\n",
      "#\t[-allname filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/allelenames File with print names for alleles\n",
      "#\t[-BA]                0                    Include Binding affinity prediction\n",
      "\n",
      "# NetMHCpan version 4.1b\n",
      "\n",
      "# Tmpdir made /tmp/netMHCpanjZrmQd\n",
      "# Input is in PEPTIDE format\n",
      "\n",
      "# Make EL predictions\n",
      "\n",
      "HLA-C16:01 : Distance to training data  0.000 (using nearest neighbor HLA-C16:01)\n",
      "\n",
      "# Rank Threshold for Strong binding peptides   0.500\n",
      "# Rank Threshold for Weak binding peptides   2.000\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      " Pos         MHC        Peptide      Core Of Gp Gl Ip Il        Icore        Identity  Score_EL %Rank_EL BindLevel\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "   1 HLA-C*16:01     YSFHHHYCNY YSFHHHYCY  0  8  1  0  0   YSFHHHYCNY         PEPLIST 0.1107560    1.291 <= WB\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Protein PEPLIST. Allele HLA-C*16:01. Number of high binders 0. Number of weak binders 1. Number of peptides 1\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "# /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/bin/netMHCpan -p /home/benji/bigmhc_rev/data/tmp/tmp.csv -a HLA-C17:01 -xls -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv\n",
      "# Wed Jan 18 14:21:23 2023\n",
      "# User: benji\n",
      "# PWD : /home/benji/bigmhc_rev/third_party/netmhcpan\n",
      "# Host: Linux orthrus1 5.10.0-19-amd64 x86_64\n",
      "# -p       1                    Use peptide input\n",
      "# -a       HLA-C17:01           MHC allele\n",
      "# -xls     1                    Save output to xls file\n",
      "# -xlsfile /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "# Command line parameters set to:\n",
      "#\t[-rdir filename]     /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64 Home directory for NetMHpan\n",
      "#\t[-syn filename]      /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/synlist.bin Synaps file\n",
      "#\t[-v]                 0                    Verbose mode\n",
      "#\t[-dirty]             0                    Dirty mode, leave tmp dir+files\n",
      "#\t[-tdir filename]     /tmp/netMHCpanXXXXXX Temporary directory (made with mkdtemp)\n",
      "#\t[-hlapseudo filename] /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/MHC_pseudo.dat File with MHC pseudo sequences\n",
      "#\t[-hlaseq filename]                        File with full length MHC sequences\n",
      "#\t[-a line]            HLA-C17:01           MHC allele\n",
      "#\t[-f filename]                             File name with input\n",
      "#\t[-w]                 0                    w option for webface\n",
      "#\t[-s]                 0                    Sort output on descending affinity\n",
      "#\t[-p]                 1                    Use peptide input\n",
      "#\t[-rth float]         0.500000             Rank Threshold for high binding peptides\n",
      "#\t[-rlt float]         2.000000             Rank Threshold for low binding peptides\n",
      "#\t[-l string]          8,9,10,11            Peptide length [8-11] (multiple length with ,)\n",
      "#\t[-xls]               1                    Save output to xls file\n",
      "#\t[-xlsfile filename]  /home/benji/bigmhc_rev/data/prd/asdf.csv Filename for xls dump\n",
      "#\t[-t float]           -99.900002           Threshold for output (%rank) [<0 print all]\n",
      "#\t[-thrfmt filename]   /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/threshold/%s.thr.%s Format for threshold filenames\n",
      "#\t[-expfix]            0                    Exclude prefix from synlist\n",
      "#\t[-version filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/version File with version information\n",
      "#\t[-inptype int]       0                    Input type [0] FASTA [1] Peptide\n",
      "#\t[-listMHC]           0                    Print list of alleles included in netMHCpan\n",
      "#\t[-allname filename]  /home/benji/bigmhc_rev/third_party/netmhcpan/Linux_x86_64/data/allelenames File with print names for alleles\n",
      "#\t[-BA]                0                    Include Binding affinity prediction\n",
      "\n",
      "# NetMHCpan version 4.1b\n",
      "\n",
      "# Tmpdir made /tmp/netMHCpanrnseA8\n",
      "# Input is in PEPTIDE format\n",
      "\n",
      "# Make EL predictions\n",
      "\n",
      "HLA-C17:01 : Distance to training data  0.000 (using nearest neighbor HLA-C17:01)\n",
      "\n",
      "# Rank Threshold for Strong binding peptides   0.500\n",
      "# Rank Threshold for Weak binding peptides   2.000\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      " Pos         MHC        Peptide      Core Of Gp Gl Ip Il        Icore        Identity  Score_EL %Rank_EL BindLevel\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "   1 HLA-C*17:01    FSDVGWNNWIV FSDVGNWIV  0  5  2  0  0  FSDVGWNNWIV         PEPLIST 0.0066400    8.753\n",
      "---------------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Protein PEPLIST. Allele HLA-C*17:01. Number of high binders 0. Number of weak binders 0. Number of peptides 1\n",
      "\n",
      "-----------------------------------------------------------------------------------\n",
      "netmhcpan 0.40022778511047363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 14:21:26.737652: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 14:21:27.591829: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-18 14:21:27.595791: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-01-18 14:21:27.771471: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "WARNING:root:No flanking information provided. Specify --no-flanking to silence this warning\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing tensorflow backend.\n",
      "Read input CSV with 3 rows, columns are: allele, peptide\n",
      "Predicting processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "/usr/local/conda/lib/python3.9/site-packages/mhcflurry/flanking_encoding.py:173: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  n_flanks = pandas.Series([\"\"] * len(df))\n",
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "100%|██████████| 1/1 [00:02<00:00,  2.59s/it]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting affinities.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/conda/lib/python3.9/site-packages/keras/engine/training_v1.py:2356: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/benji/bigmhc_rev/data/prd/asdf.csv\n",
      "mhcflurry 13.060346364974976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/benji/bigmhc_rev/third_party/mhcnuggets/mhcnuggets/src/predict.py\", line 15, in <module>\n",
      "    from mhcnuggets.src.models import get_predictions, mhcnuggets_lstm\n",
      "ModuleNotFoundError: No module named 'mhcnuggets'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Usecols do not match columns, columns expected but not found: ['ic50']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1248852/2553524310.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = run([netmhcpan, mhcflurry, mhcnuggets, mixmhcpred21, mixmhcpred22, prime1, prime2, transphla, hlathena],\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"asdf.csv\")\n",
      "\u001b[0;32m/tmp/ipykernel_1248852/2938393443.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(models, filename)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprdfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1248852/2938393443.py\u001b[0m in \u001b[0;36mmhcnuggets\u001b[0;34m(data, outfile)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mic50\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     return runmodel(\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1248852/2938393443.py\u001b[0m in \u001b[0;36mrunmodel\u001b[0;34m(func, data, name)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmhc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mhc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         out.append(pd.Series(\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmhc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pep\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             name=name))\n",
      "\u001b[0;32m/tmp/ipykernel_1248852/2938393443.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(mhc, pep)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mcmd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             cwd=os.path.join(thirdparty, \"mhcnuggets/mhcnuggets/src\"))\n\u001b[0;32m---> 59\u001b[0;31m         ic50 = pd.read_csv(\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             usecols=[\"ic50\"]).iloc[:,0].tolist()\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1235\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             ):\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_usecols_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# error: Cannot determine type of 'names'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_validate_usecols_names\u001b[0;34m(self, usecols, names)\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0mmissing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musecols\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    914\u001b[0m                 \u001b[0;34mf\"Usecols do not match columns, columns expected but not found: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m                 \u001b[0;34mf\"{missing}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Usecols do not match columns, columns expected but not found: ['ic50']"
     ]
    }
   ],
   "source": [
    "df = run([netmhcpan, mhcflurry, mhcnuggets, mixmhcpred21, mixmhcpred22, prime1, prime2, transphla, hlathena],\n",
    "    \"asdf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1403ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
